{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaidu/.pyenv/versions/3.10.11/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Let's learn about prompt engineering together!\n",
    "\"\"\"\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7XAJuWm4k3HedlSGuEABscN4pmsRq at 0x107d36c50> JSON: {\n",
       "  \"id\": \"chatcmpl-7XAJuWm4k3HedlSGuEABscN4pmsRq\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1688138850,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Prompt engineering is the process of designing and refining prompts to achieve desired outputs from language models. It involves crafting specific instructions or queries to elicit the desired response from the model. Prompt engineering is crucial in fine-tuning models and improving their performance in various tasks.\\n\\nHere are some key aspects of prompt engineering:\\n\\n1. Goal Definition: Clearly define the desired output or task you want the model to perform. This could be generating a specific type of text, answering questions, summarizing information, or any other language-related task.\\n\\n2. Context Setting: Provide relevant context or background information to guide the model's understanding. This can help the model generate more accurate and contextually appropriate responses.\\n\\n3. Instruction Clarity: Craft clear and unambiguous instructions for the model. Avoid vague or ambiguous prompts that may confuse the model and lead to incorrect or irrelevant responses.\\n\\n4. Prompt Length: Experiment with different prompt lengths to find the optimal balance. Longer prompts may provide more context but can also confuse the model. Shorter prompts may be too ambiguous and lack necessary information.\\n\\n5. Prompt Format: Explore different prompt formats such as fill-in-the-blank style, multiple-choice questions, or providing example outputs. Different formats can influence the model's behavior and generate more accurate responses.\\n\\n6. Iterative Refinement: Continuously refine and iterate on prompts based on model performance and user feedback. Experiment with different variations and analyze the model's responses to improve the prompt's effectiveness.\\n\\n7. Domain Adaptation: Tailor prompts to specific domains or topics to improve the model's performance in specialized tasks. This involves incorporating domain-specific vocabulary, examples, or instructions.\\n\\n8. Evaluation and Feedback: Regularly evaluate the model's responses and collect user feedback to identify areas for improvement. Analyze the generated outputs to identify patterns, biases, or errors that can be addressed through prompt engineering.\\n\\nBy applying these principles, prompt engineering can help optimize the performance of language models and ensure they generate more accurate, relevant, and contextually appropriate responses.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 16,\n",
       "    \"completion_tokens\": 401,\n",
       "    \"total_tokens\": 417\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
